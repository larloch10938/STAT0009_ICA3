---
output:
  html_document: default
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
```
## Student Numbers
SN: 18014015
SN: 21169367
SN: 19011782
SN: 20117921

# Introduction

* Due 9 February 2022.
* You are to work in groups of 2-4; there should be no overlap with your ICA2 groups.
* This ICA consists of five questions, worth 65 points;
* another 10 points will be given based on presentation, so that the total available points is 75.  
* This ICA is worth 30 percent of your grade for this module.
* Many questions will require you to write and run R/Python code.
* The ICA must be completed in R Markdown/Juptyer Notebook and typeset using  Markdown with Latex code, just like the way our module content is generated.  You can choose to use either R or Python.
* Please hand in the html file and the Rmd/ipynb source file.
* As usual, part of the grading will depend on the clarity and presentation of your solutions.
*  You are to do this assignment by yourselves, without any help from others.
*  You are allowed to use any materials and code that was presented so far.
*  Do not search the internet for answers to the ICA.
*  Do not use any fancy code or packages imported from elsewhere.

# Questions


## 1.) [10 points] Apery's constant 

* [2 points] Prove (using calculus) that

$$\zeta(3) = \sum_{n=1} ^{\infty} \frac{1}{n^3}  < \infty.$$

* [6 points]  Consider the probability distribution $\mu$, where 
$$\mu(n) = \frac{1}{\zeta(3)n^3}.$$
Use the Metropolis algorithm to sample from $\mu$, so that you do not need to know the value of $\zeta(3)$.

* [2 points]  Now that you can sample from $\mu$, use simulations to estimate the value of $\zeta(3)$.  


## Answer:
* We can use a Standard Series convergence test called the Integral Test (learnt in MATH0004 Analysis 2 in 1st year at UCL), which looks the series as an integral (an integral is the continuous version of a discrete sum) from 1 to $\infty$. If this definite integral converges then the series converges:
$$\int_{1}^{\infty} \frac{1}{x^{3}} \,dx \ = \lim_{t \to \infty}\int_{1}^{t} \frac{1}{x^{3}} \,dx = \lim_{t \to \infty}  \frac{1}{-2x^{2}}\Biggr|_{x=1}^{x=t} = \frac{1}{2} < \infty $$
The integral is less than $\infty$, thus $\zeta(3)$ is a convergent series as required.

* From Week 9 Exercises in class, we were introduced to the Metropolis-Hastings algorithm. 
```{r}
## Aperys constant
s = 0
for (i in 1:1000){
  s <- s + 1/(i^3)
  
}
s

#Construction of Metropolis algorithm from week9 exercises
target <- function(x){
  z=0
  if (x > 0){
    z = 1/(x^3)
  }
  z
}

Yq <- function(x){
  y = rnorm(1,x,1) ##Not sure about distribution choice here
  y
}

a <- function(j, i){
  m = min(c(target(j)/target(i),1))
  m
}

#Code idea from Week 9 exercises
metro <- function(n){
  x = 0.5
  for (k in 1:n){
    i = x[length(x)]
    j = Yq(i)
    p = a(j,i)
    x = c(x,i)
    if (rbinom(1,1,p) ==1){ ##similarly here for the distribution choice
      x[length(x)] <- j
    }
  }
  x
}

t = replicate(500, metro(500))
hist(t, prob=TRUE, breaks = 50)
curve(1/(s*(x^3)), add=TRUE) ## Actual distribution graph


```


```{r}
#Code found on the internet idea
x = rep(0,1000)
x[1] = 3     #this is just a starting value, which I've set arbitrarily to 3
for(i in 2:1000){
  currentx = x[i-1]
  proposedx = currentx + rnorm(1,mean=0,sd=1)
  A = target(proposedx)/target(currentx) 
  if(runif(1)<A){
    x[i] = proposedx       # accept move with probabily min(1,A)
  } else {
    x[i] = currentx        # otherwise "reject" move, and stay where we are
  }
}
plot(x)
hist(x, prob=TRUE, breaks = 20)
x = seq(0, 5, by = 0.1)
curve(1/(s*(x^3)), add=TRUE) ## Actual distribution graph

```

* To estimate Apery's constant with the sampling from the Metropolis-Hastings algorithm, we could simulate the distribution for all $n>0, n \in\mathbb{Z^{+}}$ and then find the sample mean of the distribution which would equate to (using the knowledge derived by Euler that $\sum_{n=1}^{\infty} \frac{1}{n^{2}}=\frac{\pi^{2}}{6}$:
$$\mathbb{E}(\mu(n))=\sum_{n=1}^{\infty} \frac{n}{\zeta(3)n^{3}} = \frac{1}{\xi(3)}\sum_{n=1}^{\infty} \frac{1}{n^{2}} = \frac{\pi^{2}}{\zeta(3)6} $$
The sample mean for this distribution will be denoted by $M$, where
$$ M = \frac{1}{N}\sum_{n=1}^{N}\mu(n),$$
where $N$ is the number of simulations we will do (in the previous part $N=500$).

So, to obtain Apery's constant,
$\zeta(3) \approx \frac{6M}{\pi^{2}}.$

USE SAMPLE MEAN OR REGULAR MEAN FROM PART 2 OF QUESTION 1?

```{r}
## code to do here
t = replicate(500, metro(500))
mean = sum(t)/500
approx = 6*mean/((pi**2))
approx
```


## 2.) [15 points] Bayesian statistics and MCMC

We say that a positive continuous random variable $X$ has the **inverse gamma** distribution with parameters $\alpha >0$ and $\beta >0$ if it has pdf given by
	$$(y; \alpha, \beta) \mapsto \frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{-\alpha -1} e^{\tfrac{-\beta}{y}}  \mathbf{1}[y >0],$$ where $\Gamma$ is the usual Gamma function.  	  

We say that a positive continuous random variable $W$ has the **Scaled-Weibull distribution** with shape parameter $k$ and scale  parameter $\theta >0$ if it has pdf given by
$$(w_1; k,\theta) \mapsto \mathbf{1}[w_1 >0]\frac{k w_1^{k-1}}{\theta}   \exp[  - \tfrac{w_1^{k}}{\theta }  ] .$$ 	

* [2 points] Let ${W} = (W_1, \ldots, W_n)$ be a random sample from the  Scaled-Weibull distribution with known shape parameter $k$ and  unknown scale parameter $\theta >0$.  Show that $t({W}) := \sum_{i=1} ^n W_i ^k$ is a sufficient statistic for $\theta$.
	
* [3 points] Fix $k >0$.   Let ${X} = (X_1, \ldots, X_n)$ be a random sample where the conditional distribution of $X_1$ given $\Theta = \theta$  has the  Scaled-Weibull distribution with shape  parameter $k$ and scale parameter $\theta$,  and $\Theta$ has the inverse gamma distribution with parameters $\alpha$ and $\beta$.   Given sample data $x=(x_1, x_2, \ldots, x_n)$.  Compute the posterior distribution $s(\theta|t(x))$ up to constant factors. 

* [3 points] Identify the distribution of $s(\theta|t(x))$.

*  [4 points] Now *pretend* you could not identify it, and could not deduce exact constant factors.   For the simple case, where $\alpha =2$,  $\beta=3$, $n=3$, and  $x_1=2, x_2=4, x_3=6$, sample from $s(\theta|t(x))$ using the Metropolis algorithm. 

*  [3 points] Plot independent samples in a probability histogram and compare with the true result.

## Answer:
*  The likelihood function of $\theta$ is $$L(\theta,k,w_i) = \prod_{i=1}^{n}\mathbf{1}[w_i >0]\frac{k w_i^{k-1}}{\theta}\exp[- \tfrac{w_i^{k}}{\theta }] = \mathbf{1}[w_{(1)} >0]k^n (\prod_{i=1}^{n} w_i^{k-1})\theta^{-n}\exp[- \tfrac{\sum_{i=1}^{n}w_i^{k}}{\theta }].$$ Then, if we define $t(W)$ as $\sum_{i=1}^{n}w_i^{k}$, we can rewrite the likelihood function as $$L(\theta,k,w_i) = \mathbf{1}[w_{(1)} >0]k^n (\prod_{i=1}^{n} w_i^{k-1})\theta^{-n}\exp[- \tfrac{t(W)}{\theta }]=g(t;\theta)*h(\underline{w}),$$ where $$g(t;\theta)=\theta^{-n}exp[-\tfrac{t(W)}{\theta}]$$ and $$h(\underline{w})=\mathbf{1}[w_{(1)}>0]k^n\prod_{i=1}^{n}w_i^{k-1}.$$ Using the factorisation theorem, we can see that  $t(W)=\sum_{i=1}^{n}w_i^{k}$ is indeed a sufficient statistic of $\theta$.

*  It is known that the posterior distribution is proportional to the product of the prior distribution and the likelihood function. Therefore, $s(\theta|t(x))$ is proportional to $$\Pi(\Theta)*L(\underline{x}|\Theta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{-\alpha -1} e^{\tfrac{-\beta}{\theta}}  \mathbf{1}[\theta >0]*\mathbf{1}[x_{(1)} >0]k^n (\prod_{i=1}^{n} x_i^{k-1})\theta^{-n}\exp[- \tfrac{t(x)}{\theta }]$$. This can be further simplified so that $$s(\theta|t(x)) = C *\frac{\beta^{\alpha}}{\Gamma(\alpha)}k^n (\prod_{i=1}^{n} x_i^{k-1})\theta^{-(\alpha+n)-1}\exp[-\tfrac{\beta+t(x)}{\theta }],$$ where $\theta >0$ and $x=(x_1, x_2, \ldots, x_n)>0$ and $C$ is a proportionality constant that would make the expression integrate to 1 (so that it is a proper probability distribution function).

*  $s(\theta|t(x))$ follows an Inverse Gamma distribution with parameters $\alpha +n$ and $\beta + t(x)$. Therefore, the proportionality constant, C, must be such that $$C *\frac{\beta^{\alpha}}{\Gamma(\alpha)}k^n (\prod_{i=1}^{n} x_i^{k-1})= \frac{[\beta + t(x)]^{\alpha+n}}{\Gamma(\alpha +n)}$$

## 3.) [15 points] A Poisson process  process on a perimeter of a semi-circle

Let $\Gamma$ be  a homogeneous Poisson point process of intensity $2$ on the upper half of the circle given by $x^2+y^2 =1$.  Here, $\Gamma$ is not the Gamma function.   Consider the point process $\Upsilon$ given by the  projection of $\Gamma$ onto the $x$-axis; that is, if $\Gamma$ had $n$ points and they are given by $(x_1, y_1), \ldots, (x_n, y_n)$, then the points of  $\Upsilon$ are just the $x$-coordinates  $x_1, \ldots, x_n$.   

*  [5 points] Write code to simulate $\Gamma$ and $\Upsilon$.    Graphically display a sample realization of these point processes.

*   [5 points] Demonstrate using simulations that $\Upsilon$ is *not* a homogeneous Poisson point process on $[-1,1]$.

*  [5 points]  Show analytically that $\Upsilon$ cannot be a homogeneous Poisson point process on $[-1,1]$.
```{r}
#3.1
#function to define point on the upper circle
point <- function(){
  z <- 2
    x <- 2*runif(1) -1
    y <- sqrt(1-x^2)
    z <- c(x,y)
  z
}
#replicate to create multiple points
re <- replicate(rpois(1,2*pi),point())

#plot upper half circle for ease of interpretation
curve(sqrt(1-x^2), from=-1, to=1 ,lty=2, xlab="x", ylab="y")

#plot points to get Gamma
points(re[1,], re[2,], xlim=c(-1.1,1.1), ylim=c(-0.1,1.1))

#plot zero line for ease of interpretation
abline(0,0,lty=2)

#plot to get projection onto x axis, this is Upsilon
points(re[1,],rep(0,length(re[1,])))

#3.2
#To show non-homogeneity, replicate this 1000 times, split into 2 intervals, take
#means, means should differ. See ICA 1.
```

## 4.)  [15 points] The transition rate matrix

You are given the the sample data from an irreducible  continuous-time Markov chain.  The sample data includes the jump times $(0,j_1, \ldots, j_n)$ and states $(s_0, s_1, \ldots, s_n)$; here at time $j_i$ the Markov chain jumps into state $s_i$ and stays there until the next jump which occurs at time $j_{i+1}$.

* [8 points]  When $n$ is large, give a method for estimating the transition rate matrix, also referred to as the $Q$ matrix.  Explain why your estimate is reasonable.

* [7 points] Import the data from the file [Q.txt](https://tsoo-math.github.io/ucl2/Q.txt) and use this data and your method above to estimate the $Q$ matrix.

## Answer:

When $n$ is large we can use the generated data to compute the average amount of time spent in a specific state. We can also compute the average amount of time spent in a state before jumping to another specific state. We can define the jumping time as the product of the intensity of the jumping process in one state and the probability of moving into that specific state.

We define $t(i, j) = \lambda(i)m_{i,j}$ where $\lambda(i)$ is the intensity of the process in state $i$ and $m_{i,j}$ is the transition probability. Since we are able to obtain the long run $t(i, j)$ jumping times we can them use to compute $m_{i,j}$ and plugging those into the formula seen in lesson (https://tsoo-math.github.io/ucl/continuous-timeMC.html) we can derive $q_{i,j}$ and therefore the overall $Q$ matrix.

Finally if we normalize the amount of time spent in each state we will be able to get the stationary distribution. We can than use the $\pi Q = 0$ to verify our results.

```{r, message=FALSE}
library(tidyverse)
t <- matrix(data=list(),nrow=3,ncol=3)
m <- matrix(data=list(),nrow=3,ncol=3)
q <- matrix(data=list(),nrow=3,ncol=3)
```

```{r}
data <- read.table(url("https://tsoo-math.github.io/ucl2/Q.txt"), header = TRUE, sep = ",") %>% 
  mutate(previous_state = lag(states)) %>% 
  mutate(jump_time = time - lag(time))
head(data)
# by running the below code we can see we have three states
state_list <- data %>%
  distinct(states) %>% 
  pull()
# by running the below code we get the amount of time spent on each state
data %>% 
  group_by(previous_state) %>% 
  summarize(mean(jump_time)) %>% 
  drop_na()
# what is the average time to go from state i to state j?
for (i in state_list) {
  for (j in state_list){
    avg_time <- data %>% 
      filter(states == j & previous_state == i) %>% 
      group_by(states, previous_state) %>% 
      summarize(mean(jump_time), .groups = 'drop') %>% 
      ungroup() %>% 
      pull("mean(jump_time)")
    t[[i,j]] <- avg_time
  }
}
print(paste0("Average time spent jumping from state to state:"))
print(t)
for (n in state_list) {
  t_1 <- if(n == 1) {0} else {t[[n, 5 - n]]}
  t_2 <- if(n == 2) {0} else {t[[n, 4 - n]]}
  t_3 <- if(n == 3) {0} else {t[[n, 3 - n]]}
  m[[n,1]] <- if(n == 1) {0} else {1 / (1 + t_1/t[[n,1]])}
  m[[n,2]] <- if(n == 2) {0} else {1 / (1 + t_2/t[[n,2]])}
  m[[n,3]] <- if(n == 3) {0} else {1 / (1 + t_3/t[[n,3]])}
}
print(m)
```


##  5.) [10 points] Queues 

Suppose you  have Poisson arrivals, with intensity $6$.  You are given the following two options. Option 1: we treat it like a $M(6)/M(8)/1$ system- the items are served by exponentially at rate $8$.  Option 2:  each item is painted red or blue independently with probability $\tfrac{1}{2}$;  the coloured items report to different queues, with  the red items are served exponentially at rate $4$, and the blue items served exponentially at rate $4$.  

* [5 points] Run simulations to identify   the stationary distributions of the items in each of the two options.    Which option, on average, has more items in it?

*  [5 points] Which option is better, from the items/customers perspective?  Explain, analytically.

```{r}
opt <- function(t){
  inter = rexp(1,6)
  arr = inter
  
  while(t > arr[length(arr)]){
    inter <- c(inter, rexp(1,6))
    arr <- cumsum(inter)
  }
  
  L = length(inter)
  service = rexp(L,8)
  
  output <- arr[1] + service[1]
  for (i in 1:(L - 1)){ 
    if(arr[i + 1] < output[i]){output <- c(output, output[i] + service[i + 1])}
    if(arr[i + 1] > output[i]){output <- c(output, arr[i + 1] + service[i + 1])}
  }
  
  output <- output[-L]

  n = sum(output > t)
  
}
x= replicate(1000, opt(600))
b = seq(-1,max(x)+1, by=1)
hist(x, prob=TRUE, breaks=b)
p=1-0.75
mean(x) - (  (1-p)/p )
testt1= c(sum(x==0)/1000 - dgeom(0,p),sum(x==1)/1000 - dgeom(1,p)
          ,sum(x==2)/1000 - dgeom(2,p),sum(x==3)/1000 - dgeom(3,p),
          sum(x==4)/1000 - dgeom(4,p))

opt2 <- function(t,n){
  inter = rexp(1,6)
  arr = inter
  
  while(t > arr[length(arr)]){
    inter <-c(inter, rexp(1,1))
    arr <- cumsum(inter)
  }
  L = length(inter)
  
  item <- sample(c(0,1), n, replace = TRUE, prob = c(0.5,0.5))
  for (j in 1:n){
    
  while (item[j] == 0){service1 = rexp(L, 4)}
  output1 <- arr[1] + service1[1]
  for (i in 1:(L - 1)){  
    if(arr[i + 1] < output1[i]){output1 <- c(output1, output1[i] + service1[i + 1])}
    if(arr[i + 1] > output1[i]){output1 <- c(output1, arr[i + 1] + service1[i + 1])}
  }
  
  while (item[j] == 1){service2 = rexp(L, 4)}
  output2 <- arr[1] + service2[1]
  for (i in 1:(L - 1)){ 
    if(arr[i + 1] < output2[i]){output2 <- c(output2, output2[i] + service2[i + 1])}
    if(arr[i + 1] > output2[i]){output2 <- c(output2, arr[i + 1] + service2[i + 1])}
  }
  }
  output1 <- output1[-L]
  output2 <- output2[-L]
  
  n1 = sum(output1 > t)
  n2 = sum(output2 > t)
  c(n1,n2)
}
```

# Endnotes

* Version: `r format(Sys.time(), '%d %B %Y')`
* [Rmd Source](https://tsoo-math.github.io/ucl2/2021-ica3-stat9-release.Rmd)


