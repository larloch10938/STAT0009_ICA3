---
output:
  html_document: default
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
```
## Student Numbers
SN: 18014015
SN: 21169367
SN: 19011782
SN: 20117921

# Introduction

* Due 9 February 2022.
* You are to work in groups of 2-4; there should be no overlap with your ICA2 groups.
* This ICA consists of five questions, worth 65 points;
* another 10 points will be given based on presentation, so that the total available points is 75.  
* This ICA is worth 30 percent of your grade for this module.
* Many questions will require you to write and run R/Python code.
* The ICA must be completed in R Markdown/Juptyer Notebook and typeset using  Markdown with Latex code, just like the way our module content is generated.  You can choose to use either R or Python.
* Please hand in the html file and the Rmd/ipynb source file.
* As usual, part of the grading will depend on the clarity and presentation of your solutions.
*  You are to do this assignment by yourselves, without any help from others.
*  You are allowed to use any materials and code that was presented so far.
*  Do not search the internet for answers to the ICA.
*  Do not use any fancy code or packages imported from elsewhere.

# Questions


## 1.) [10 points] Apery's constant 

* [2 points] Prove (using calculus) that

$$\zeta(3) = \sum_{n=1} ^{\infty} \frac{1}{n^3}  < \infty.$$

* [6 points]  Consider the probability distribution $\mu$, where 
$$\mu(n) = \frac{1}{\zeta(3)n^3}.$$
Use the Metropolis algorithm to sample from $\mu$, so that you do not need to know the value of $\zeta(3)$.

* [2 points]  Now that you can sample from $\mu$, use simulations to estimate the value of $\zeta(3)$.  


## Answer:
* We can use a Standard Series convergence test called the Integral Test (learnt in MATH0004 Analysis 2 in 1st year at UCL), which looks at the series as an integral (an integral is the continuous version of a discrete sum) from 1 to $\infty$. If this definite integral converges then the series converges:
$$\int_{1}^{\infty} \frac{1}{x^{3}} \,dx \ = \lim_{t \to \infty}\int_{1}^{t} \frac{1}{x^{3}} \,dx = \lim_{t \to \infty}  \frac{1}{-2x^{2}}\Biggr|_{x=1}^{x=t} = \frac{1}{2} < \infty $$
The integral is less than $\infty$, thus $\zeta(3)$ is a convergent series as required.

* From Week 9 Exercises in class, we were introduced to the Metropolis-Hastings algorithm. 
```{r}
## Aperys constant for verification
s = 0
for (i in 1:1000){
  s <- s + 1/(i^3)
  
}
s

#Construction of Metropolis algorithm from week 9 exercises

pi <- function(x){ #h function is the target distribution we have up to a constant
 z=0
  if(x >0){z = 1/(x^3)} #target distribution up to a constant
 z
  }


Yq <- function(x){ #Yq function calculates the proposal distribution for each x
   y= rnorm(1,x,1) #proposal distribution
y
}

alpha <- function(j,i){ #alpha function works out the probability of moving from state i to  state j
m=min( c( pi(j)/pi(i), 1 ))  #comparison of the proposal distribution ratio and 1
m
}

metro <- function(n){ #the metropolis-hastings algorithm function combining everything together to find the samples of the mu distribution
  x=10
  for(k in 1:n){
    i = x[length(x)]
    j = Yq(i)
    p = alpha(j,i) #the probability of moving from state i to state j
    x = c(x,i)
      if ( rbinom(1,1,p)==1 ) {
      x[length(x)] <- j
            }
  }
  x
}


z = replicate(1000, metro(1000)) #run the algorithm 1000 times to converge to the target distribution

hist(z, prob=TRUE, breaks=100, to = 30) #plot of the distribution as probability histograms
x= seq(0, 1, by =0.1)
curve(1/(s*(x^3)), add=TRUE, to = 30) #the actual distribution as verification with Apery's constant

```


* To estimate Apery's constant with the sampling from the Metropolis-Hastings algorithm, we could simulate the distribution for all $n>0, n \in\mathbb{Z^{+}}$ and then find the sample mean of the distribution which would equate to (using the knowledge derived by Euler that $\sum_{n=1}^{\infty} \frac{1}{n^{2}}=\frac{\pi^{2}}{6}$:
$$\mathbb{E}(\mu(n))=\sum_{n=1}^{\infty} \frac{n}{\zeta(3)n^{3}} = \frac{1}{\xi(3)}\sum_{n=1}^{\infty} \frac{1}{n^{2}} = \frac{\pi^{2}}{\zeta(3)6} $$
The sample mean for this distribution will be denoted by $M$, where
$$ M = \frac{1}{N}\sum_{n=1}^{N}\mu(n),$$
where $N$ is the number of simulations we will do (in the previous part $N=500$).

So, to obtain Apery's constant,
$\zeta(3) \approx \frac{\pi^{2}}{6M}.$

USE SAMPLE MEAN OR REGULAR MEAN FROM PART 2 OF QUESTION 1?

```{r}
## code to do here

mean(z) 
approx = (pi^2)/(6*mean(z))
approx
```



## 2.) [15 points] Bayesian statistics and MCMC

We say that a positive continuous random variable $X$ has the **inverse gamma** distribution with parameters $\alpha >0$ and $\beta >0$ if it has pdf given by
	$$(y; \alpha, \beta) \mapsto \frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{-\alpha -1} e^{\tfrac{-\beta}{y}}  \mathbf{1}[y >0],$$ where $\Gamma$ is the usual Gamma function.  	  

We say that a positive continuous random variable $W$ has the **Scaled-Weibull distribution** with shape parameter $k$ and scale  parameter $\theta >0$ if it has pdf given by
$$(w_1; k,\theta) \mapsto \mathbf{1}[w_1 >0]\frac{k w_1^{k-1}}{\theta}   \exp[  - \tfrac{w_1^{k}}{\theta }  ] .$$ 	

* [2 points] Let ${W} = (W_1, \ldots, W_n)$ be a random sample from the  Scaled-Weibull distribution with known shape parameter $k$ and  unknown scale parameter $\theta >0$.  Show that $t({W}) := \sum_{i=1} ^n W_i ^k$ is a sufficient statistic for $\theta$.
	
* [3 points] Fix $k >0$.   Let ${X} = (X_1, \ldots, X_n)$ be a random sample where the conditional distribution of $X_1$ given $\Theta = \theta$  has the  Scaled-Weibull distribution with shape  parameter $k$ and scale parameter $\theta$,  and $\Theta$ has the inverse gamma distribution with parameters $\alpha$ and $\beta$.   Given sample data $x=(x_1, x_2, \ldots, x_n)$.  Compute the posterior distribution $s(\theta|t(x))$ up to constant factors. 

* [3 points] Identify the distribution of $s(\theta|t(x))$.

*  [4 points] Now *pretend* you could not identify it, and could not deduce exact constant factors.   For the simple case, where $\alpha =2$,  $\beta=3$, $n=3$, and  $x_1=2, x_2=4, x_3=6$, sample from $s(\theta|t(x))$ using the Metropolis algorithm. 

*  [3 points] Plot independent samples in a probability histogram and compare with the true result.

## Answer:
*  The likelihood function of $\theta$ is $$L(\theta,k,w_i) = \prod_{i=1}^{n}\mathbf{1}[w_i >0]\frac{k w_i^{k-1}}{\theta}\exp[- \tfrac{w_i^{k}}{\theta }] = \mathbf{1}[w_{(1)} >0]k^n (\prod_{i=1}^{n} w_i^{k-1})\theta^{-n}\exp[- \tfrac{\sum_{i=1}^{n}w_i^{k}}{\theta }].$$ Then, if we define $t(W)$ as $\sum_{i=1}^{n}w_i^{k}$, we can rewrite the likelihood function as $$L(\theta,k,w_i) = \mathbf{1}[w_{(1)} >0]k^n (\prod_{i=1}^{n} w_i^{k-1})\theta^{-n}\exp[- \tfrac{t(W)}{\theta }]=g(t;\theta)*h(\underline{w}),$$ where $$g(t;\theta)=\theta^{-n}exp[-\tfrac{t(W)}{\theta}]$$ and $$h(\underline{w})=\mathbf{1}[w_{(1)}>0]k^n\prod_{i=1}^{n}w_i^{k-1}.$$ Using the factorisation theorem, we can see that  $t(W)=\sum_{i=1}^{n}w_i^{k}$ is indeed a sufficient statistic of $\theta$.

*  It is known that the posterior distribution is proportional to the product of the prior distribution and the likelihood function. Therefore, $s(\theta|t(x))$ is proportional to $$\Pi(\Theta)\times L(\underline{x}|\Theta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{-\alpha -1} e^{\tfrac{-\beta}{\theta}}  \mathbf{1}[\theta >0]\times\mathbf{1}[x_{(1)} >0]k^n (\prod_{i=1}^{n} x_i^{k-1})\theta^{-n}\exp[- \tfrac{t(x)}{\theta }]$$. This can be further simplified so that $$s(\theta|t(x)) = C \times\frac{\beta^{\alpha}}{\Gamma(\alpha)}k^n (\prod_{i=1}^{n} x_i^{k-1})\theta^{-(\alpha+n)-1}\exp[-\tfrac{\beta+t(x)}{\theta }],$$ where $\theta >0$ and $x=(x_1, x_2, \ldots, x_n)>0$ and $C$ is a proportionality constant that would make the expression integrate to 1 (so that it is a proper probability distribution function).

*  $s(\theta|t(x))$ follows an Inverse Gamma distribution with parameters $\alpha +n$ and $\beta + t(x)$. Therefore, the proportionality constant, C, must be such that $$C *\frac{\beta^{\alpha}}{\Gamma(\alpha)}k^n (\prod_{i=1}^{n} x_i^{k-1})= \frac{[\beta + t(x)]^{\alpha+n}}{\Gamma(\alpha +n)}.$$

## 3.) [15 points] A Poisson process  process on a perimeter of a semi-circle

Let $\Gamma$ be  a homogeneous Poisson point process of intensity $2$ on the upper half of the circle given by $x^2+y^2 =1$.  Here, $\Gamma$ is not the Gamma function.   Consider the point process $\Upsilon$ given by the  projection of $\Gamma$ onto the $x$-axis; that is, if $\Gamma$ had $n$ points and they are given by $(x_1, y_1), \ldots, (x_n, y_n)$, then the points of  $\Upsilon$ are just the $x$-coordinates  $x_1, \ldots, x_n$.   

*  [5 points] Write code to simulate $\Gamma$ and $\Upsilon$.    Graphically display a sample realization of these point processes.

*   [5 points] Demonstrate using simulations that $\Upsilon$ is *not* a homogeneous Poisson point process on $[-1,1]$.

*  [5 points]  Show analytically that $\Upsilon$ cannot be a homogeneous Poisson point process on $[-1,1]$.

## Answer:

#### 3.1
```{r}
#3.1
#function to define point on the upper circle
point <- function(){
    x <- 2*runif(1) -1
    y <- sqrt(1-x^2)
    z <- c(x,y)
  z
}
#replicate to create multiple points
re <- replicate(rpois(1,2*pi),point())

#plot upper half circle for ease of interpretation
curve(sqrt(1-x^2), from=-1, to=1 ,lty=2, xlab="x", ylab="y")

#plot points to get Gamma
points(re[1,], re[2,], xlim=c(-1.1,1.1), ylim=c(-0.1,1.1))

#plot zero line for ease of interpretation
abline(0,0,lty=2)

#plot to get projection onto x axis, this is Upsilon
points(re[1,],rep(0,length(re[1,])))
```

#### 3.2
Remembering the last exercise (shop keeper) from ICA1, this problem can be set up similarly, using the law of large numbers. The law of large numbers tells us that $${\frac {1}{n} (X_1+...+X_n) \rightarrow E[X_1]= \lambda (s-r) \; \text{as} \; n \rightarrow \infty},$$ where (s-r) is the length of the interval which $\lambda$ is estimated for. Therefore, the estimator $\frac {1}{n(s-r)} (X_1+...+X_n)$ is consistent and unbiased for $\lambda$, the true rate of the Poisson process on [r,s]. With this estimator and the sample data, we give the corresponding estimate $\frac {1}{n(s-r)} (x_1+...+x_n)$ for $\lambda$.

Then, if we repeat the point generating-process from *3.1* n-times for $\Upsilon$, and we let $X_1,...,X_n$ be random variables representing the number of points generated for each of the $n$ replicates, and given that we can assume that these random variables are independent, we can say that the model gives that $X_i$ is a Poisson random variable with mean $\lambda_i$.  

Thus, if $[-1,1]$ interval is split up into smaller intervals, and the rate ($\lambda_i$) of the Poisson process $X_i$ is estimated in each interval, the result should be different in each interval, if $\Upsilon$ is indeed a non-homogeneous Poisson point process on $[-1,1]$.

We can estimate $\lambda_i$, the rate of "arrival" in the $i^th$ interval for all i, where in our case $i=1,2,3$. Then, if the estimates in each interval are approximately equal, $\Upsilon$ is homogeneous, and if the estimates are unequal, $\Upsilon$ is non-homogeneous. This is because as $n \rightarrow \infty$, we know that our estimator of $\lambda_i$ will be unbiased and consistent. 

The below code chunk will execute this in R.
```{r}
#3.2
#First, we generated points for Upsilon 100 times and created a vector, "reps" to
#store all the points.
set.seed(123)
counter<- 0
reps <- 0
n <- 100
while(counter < n){
  counter <- counter+1
  reps <- append(reps,replicate(rpois(1,2*pi),point())[1,])
}

#Then, the function "nonhom" was defined, that splits the [-1,1] into 3 intervals at
#x and y, and estimates lambda_i in each interval according to the method described 
#above, then outputs these estimates.

#As it is not specified whether x<y or y<x in the function, the three intervals are
#[-1,min(x,y)], [min(x,y),max(x,y)], and [max(x,y),1].

nonhom <- function(x,y){     
          int1 <- reps[reps<min(x,y)]            #define points in first interval  
          int2 <- reps[min(x,y)<reps && reps <max(x,y)] #define points in second interval
          int3 <- reps[reps>max(x,y)]                   #define points third interval
          if (length(int1) > 0){
          numpoint1 <- length(int1)   #If there are points in the interval, count
          } else {                    #the number of points in it. 
            numpoint1 <- 0            #If the interval is empty, the number of points
          }                           #is 0.
          if (length(int2) > 0){      #Repeat this for all three intervals.
          numpoint2 <- length(int2)
          } else {
            numpoint2 <- 0
          }
          if (length(int3) > 0){
          numpoint3 <- length(int3)
          } else {
            numpoint3 <- 0
          }                           #estimate each lambda according to the
                                      #formula defined above and print results
          
          print(c(numpoint1*1/(n*(min(x,y)+1)),
                  numpoint2*1/(n*(abs(x-y))), 
                  numpoint3*1/(n*(1-max(x,y)))))
}

#The values -0.4 and 0.3 were randomly selected. From the results it is clear that three lambda values are different, meaning that Upsilon is indeed non-homogeneous.

nonhom(-0.4,0.3) #this is lambda1, lambda2, and lambda3

#Thus, we proved that Upsilon is a non-homogeneous Poisson point process.
```
#### 3.3
Prove analytically

## 4.)  [15 points] The transition rate matrix

You are given the the sample data from an irreducible  continuous-time Markov chain.  The sample data includes the jump times $(0,j_1, \ldots, j_n)$ and states $(s_0, s_1, \ldots, s_n)$; here at time $j_i$ the Markov chain jumps into state $s_i$ and stays there until the next jump which occurs at time $j_{i+1}$.

* [8 points]  When $n$ is large, give a method for estimating the transition rate matrix, also referred to as the $Q$ matrix.  Explain why your estimate is reasonable.

* [7 points] Import the data from the file [Q.txt](https://tsoo-math.github.io/ucl2/Q.txt) and use this data and your method above to estimate the $Q$ matrix.

## Answer:

When $n$ is large we can use the generated data to compute the average amount of time spent in a specific state. This is $h$.

When $n$ is large we can also compute the average probability of a jump from one state to the other. By computing this we can find an empiric definition of $M$.

Using the formula defined in our lessons (https://tsoo-math.github.io/ucl/continuous-timeMC.html) we can derive $q_{i,i}$.

Combing $M$ and the derived $q_{i,i}$ will eventually result in $Q$.

```{r, message=FALSE}
library(tidyverse)
m <- matrix(data=0,nrow=3,ncol=3)
q <- matrix(data=0,nrow=3,ncol=3)
```

```{r, message=FALSE}
data <- read.table(url("https://tsoo-math.github.io/ucl2/Q.txt"), header = TRUE, sep = ",") %>% 
  mutate(previous_state = lag(states)) %>% 
  mutate(jump_time = time - lag(time))
# by running the below code we can see we have three states
state_list <- data %>%
  distinct(states) %>% 
  pull()
# by running the below code we get the amount of time spent on each state
h <- data %>% 
  group_by(previous_state) %>% 
  summarize(mean(jump_time)) %>% 
  drop_na() %>% 
  pull('mean(jump_time)')
print(h)
# compute q_ii from h
q_ii <- -1 / h
print(q_ii)
# by running the below code we get the amount of times we jumped from state i to state j
m_data <- data %>% 
  group_by(states, previous_state) %>% 
  summarize(n = n())
for (i in state_list) {
  for (j in state_list) {
    if (i != j) {
      m[i, j] <- m_data %>% 
      filter(states == j) %>% 
      filter(previous_state == i) %>% 
      pull(n)
    }
  }
}
m <- t(apply(m, 1, function(x) x/sum(x)))
print(m)
for (i in state_list) {
  q[i, ] <- -m[i, ] * q_ii[i]
  q[i, i] <- q_ii[i]
}
print(q)
```


##  5.) [10 points] Queues 

Suppose you  have Poisson arrivals, with intensity $6$.  You are given the following two options. Option 1: we treat it like a $M(6)/M(8)/1$ system- the items are served by exponentially at rate $8$.  Option 2:  each item is painted red or blue independently with probability $\tfrac{1}{2}$;  the coloured items report to different queues, with  the red items are served exponentially at rate $4$, and the blue items served exponentially at rate $4$.  

* [5 points] Run simulations to identify   the stationary distributions of the items in each of the two options.    Which option, on average, has more items in it?

*  [5 points] Which option is better, from the items/customers perspective?  Explain, analytically.

The two options are very similar: both receive the arrivals at the same rate but serves them in a different way (i.e. as a $M(8)$ versus 2 $M(4)$). We can clearly see the results will be different.

Option 1 is clearly superior to Option 2 since the amount of expected customers will be the same and they will all be served in the same queue; as soon as one customer is exiting the queue the next one will be served. That is not true for option 2, which is therefore prone to dead times and will be less efficient.

If we treat Option 2 as a combination of two $M(3)/M(4)/1$ system given the probability to get a red or blue ball is independent we can compute the expected time in the system using Little's Law as:

$$E(W) = \frac{1}{\lambda - \mu} = \frac{1}{4 - 3} = 1$$
while in option 1 we have:

$$E(W) = \frac{1}{\lambda - \mu} = \frac{1}{8 - 6} = \frac{1}{2}$$

```{r}
num <- function(n) {
  M_1 <- 6
  M_2 <- 8
  inter <- rexp(n, M_1)
  arr <- cumsum(inter)
  service <- rexp(n, M_2)
  output <- arr[1] + service[1]
  for (i in 1:(n - 1)) {
      output <- c(output, max(output[i], arr[i + 1]) + service[i + 1])
  }
  return(mean(output - arr))
}

mean(replicate(1000, num(1000)))
```


```{r, eval=FALSE}
 opt <- function(t){
  inter = rexp(1,6)
  arr = inter
  
  while(t > arr[length(arr)]){  ## x[length(x)] is to extract the last data in x
    inter <- c(inter, rexp(1,6))
    arr <- cumsum(inter)
  }
  
  L = length(inter)
  service = rexp(L,8)
  
  output <- arr[1] + service[1]
  for (i in 1:(L - 1)){  ## whether the server's service time less than the customer arrival time 
    if(arr[i + 1] < output[i]){output <- c(output, output[i] + service[i + 1])}
    if(arr[i + 1] > output[i]){output <- c(output, arr[i + 1] + service[i + 1])}
  }
  
  output <- output[-L] ## delete output[L]

  n = sum(output > t)
  
}
x= replicate(1000, opt(600)) ## simulation by large number
b = seq(-1,max(x)+1, by=1)
hist(x, prob=TRUE, breaks=b) ## draw a diagrom to assume the distribution
p=1-0.75
mean(x) - (  (1-p)/p )
testt1= c(sum(x==0)/1000 - dgeom(0,p),sum(x==1)/1000 - dgeom(1,p)
          ,sum(x==2)/1000 - dgeom(2,p),sum(x==3)/1000 - dgeom(3,p),
          sum(x==4)/1000 - dgeom(4,p))
testt1
opt2 <- function(t){
  inter = rexp(1,6)
  arr = inter
  
  while(t > arr[length(arr)]){
    inter <- c(inter, rexp(1,6))
    arr <- cumsum(inter)
  }
  L = length(inter)
  
  colitem = sample(c(0,1), length(arr), replace = TRUE, prob = c(0.5,0.5))
  
  arrred <- 0
  arrblue <- 0
  for (i in 1:L) {
 
    if(colitem[i] == 0){arrred <- c(arrred, arr[i])}
    if(colitem[i] == 1){arrblue <- c(arrblue, arr[i])}
  }     
    arrred
    arrblue
    
  Lr = length(arrred)
  servicered = rexp(Lr,4)
  
  outputred <- arrred[1] + servicered[1]
  for (j in 1:(Lr - 1)){ 
    if(arrred[j + 1] < outputred[j]){outputred <- c(outputred, outputred[j] + servicered[j + 1])}
    if(arrred[j + 1] > outputred[j]){outputred <- c(outputred, arrred[j + 1] + servicered[j + 1])}
  }
  outputred <- outputred[-L]  
  
  Lb = length(arrblue)
  serviceblue = rexp(Lb,4)  
  outputblue <- arrblue[1] + serviceblue[1]
  for (m in 1:(Lb - 1)){  
    if(arrblue[m + 1] < outputblue[m]){outputblue <- c(outputblue, outputblue[m] + serviceblue[m + 1])}
    if(arrblue[m + 1] > outputblue[m]){outputblue <- c(outputblue, arrblue[m + 1] + serviceblue[m + 1])}
  }
  outputblue <- outputblue[-L]  
  
  nr = sum(outputred > t)
  nb = sum(outputblue > t)
  
  c(nr,nb)
}


y= replicate(1000, opt2(600)) ## simulation by large number
y1 = y[1,]
y2 = y[2,]

a = seq(-1,max(y)+1, by=1)
b1 = seq(-1,max(y1)+1, by=1)
b2 = seq(-1,max(y2)+1, by=1)

hist(y, prob=TRUE, breaks=a)
hist(y1, prob=TRUE, breaks=b1) ## draw a diagrom to assume the distribution
hist(y2, prob=TRUE, breaks=b1)

q=1-0.75
mean(y) - ( (1-q)/q )
testt2= c(sum(y==0)/1000 - dgeom(0,q),sum(y==1)/1000 - dgeom(1,q)
          ,sum(y==2)/1000 - dgeom(2,p),sum(y==3)/1000 - dgeom(3,q),
          sum(y==4)/1000 - dgeom(4,q))
testt2

```

# Endnotes

* Version: `r format(Sys.time(), '%d %B %Y')`
* [Rmd Source](https://tsoo-math.github.io/ucl2/2021-ica3-stat9-release.Rmd)


