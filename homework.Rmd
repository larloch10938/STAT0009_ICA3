```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
# py_install("matplotlib")
```
## Student Numbers
SN: 18014015
SN: 21169367
sN: 19011782
SN:
# Introduction

* Made available UCL Week 16
* Due UCL Week 24; 9 February 2022.
* You are to work in groups of 2-4; there should be no overlap with your ICA2 groups.
* This ICA consists of five questions, worth 65 points;
* another 10 points will be given based on presentation, so that the total available points is 75.  
* This ICA is worth 30 percent of your grade for this module.
* Many questions will require you to write and run R/Python code.
* The ICA must be completed in R Markdown/Juptyer Notebook and typeset using  Markdown with Latex code, just like the way our module content is generated.  You can choose to use either R or Python.
* Please hand in the html file and the Rmd/ipynb source file.
* As usual, part of the grading will depend on the clarity and presentation of your solutions.
*  You are to do this assignment by yourselves, without any help from others.
*  You are allowed to use any materials and code that was presented so far.
*  Do not search the internet for answers to the ICA.
*  Do not use any fancy code or packages imported from elsewhere.

## Anonymous Marking 

Please do **not** write your name anywhere on the submission. Please include only your **student number** as the proxy identifier.

# Questions


## 1.) [10 points] Apery's constant 

* [2 points] Prove (using calculus) that

$$\zeta(3) = \sum_{n=1} ^{\infty} \frac{1}{n^3}  < \infty.$$

* [6 points]  Consider the probability distribution $\mu$, where 
$$\mu(n) = \frac{1}{\zeta(3)n^3}.$$
Use the Metropolis algorithm to sample from $\mu$, so that you do not need to know the value of $\zeta(3)$.

* [2 points]  Now that you can sample from $\mu$, use simulations to estimate the value of $\zeta(3)$.  


## Answer:
* We can use a Standard Series convergence test called the Integral Test (learnt in MATH0004 Analysis 2 in 1st year at UCL), which looks the series as an integral (an integral is the continuous version of the discrete sum) from 1 to $\infty$. If this definite integral converges then the series converges:
$$\int_{1}^{\infty} \frac{1}{x^{3}} \,dx \ = \lim_{t \to \infty}\int_{1}^{t} \frac{1}{x^{3}} \,dx = \lim_{t \to \infty}  \frac{1}{-2x^{2}}\Biggr|_{x=1}^{t} = \frac{1}{2} < \infty $$
The integral is less than $\infty$, thus $\zeta(3)$ is a convergent series as required.

* From Week 9 Exercises in class, we were introduced to the Metropolis-Hastings algorithm. 
```{r}
## Aperys constant
s = 0
for (i in 1:1000){
  s <- s + 1/(i^3)
  
}
s


target <- function(x){
  z=0
  if (x > 0){
    z = 1/(x^3)
  }
  z
}

Yq <- function(x){
  y = rnorm(1,x,1) ##Not sure about distribution choice here
  y
}

a <- function(j, i){
  m = min(c(target(j)/target(i),1))
  m
}

x = rep(0,1000)
x[1] = 3     #this is just a starting value, which I've set arbitrarily to 3
for(i in 2:1000){
  currentx = x[i-1]
  proposedx = currentx + rnorm(1,mean=0,sd=1)
  A = target(proposedx)/target(currentx) 
  if(runif(1)<A){
    x[i] = proposedx       # accept move with probabily min(1,A)
  } else {
    x[i] = currentx        # otherwise "reject" move, and stay where we are
  }
}
plot(x)
hist(x)
x = seq(0, 5, by = 0.1)
curve(1/(s*(x^3)), add=TRUE) ## Actual distribution graph

```


```{r}
metro <- function(n){
  x = 0.5
  for (k in 1:n){
    i = x[length(x)]
    j = Yq(i)
    p = a(j,i)
    x = c(x,i)
    if (rbinom(1,1,p) ==1){ ##similarly here for the distribution choice
      x[length(x)] <- j
    }
  }
  x
}

t = replicate(500, metro(500))
hist(t, prob=TRUE, breaks = 50)

x = seq(0, 5, by = 0.1)
curve(1/(s*(x^3)), add=TRUE) ## Actual distribution graph

```

* To estimate Apery's constant with the sampling from the Metropolis-Hastings algorithm, we could simulate the distribution for all $n>0$ and then find the sample mean of the distribution which would equate to:
$$\int_{1}^{\infty} \frac{x}{\zeta(3) x^{3}} \,dx = \lim_{t \to \infty}\int_{1}^{t} \frac{1}{\zeta(3) x^{2}} \,dx = \lim_{t \to \infty}  \frac{1}{-\zeta(3)x}\Biggr|_{x=1}^{t} = -\frac{1}{\zeta(3)}.$$

So, the sample mean of the distribution is the reciprocal of Apery's constant?

```{r}
## code to do here
t = replicate(500, metro(500))
mean = sum(t)/500
1/mean
```


## 2.) [15 points] Bayesian statistics and MCMC

We say that a positive continuous random variable $X$ has the **inverse gamma** distribution with parameters $\alpha >0$ and $\beta >0$ if it has pdf given by
	$$(y; \alpha, \beta) \mapsto \frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{-\alpha -1} e^{\tfrac{-\beta}{y}}  \mathbf{1}[y >0],$$ where $\Gamma$ is the usual Gamma function.  	  

We say that a positive continuous random variable $W$ has the **Scaled-Weibull distribution** with shape parameter $k$ and scale  parameter $\theta >0$ if it has pdf given by
$$(w_1; k,\theta) \mapsto \mathbf{1}[w_1 >0]\frac{k w_1^{k-1}}{\theta}   \exp[  - \tfrac{w_1^{k}}{\theta }  ] .$$ 	

* [2 points] Let ${W} = (W_1, \ldots, W_n)$ be a random sample from the  Scaled-Weibull distribution with known shape parameter $k$ and  unknown scale parameter $\theta >0$.  Show that $t({W}) := \sum_{i=1} ^n W_i ^k$ is a sufficient statistic for $\theta$.
	
* [3 points] Fix $k >0$.   Let ${X} = (X_1, \ldots, X_n)$ be a random sample where the conditional distribution of $X_1$ given $\Theta = \theta$  has the  Scaled-Weibull distribution with shape  parameter $k$ and scale parameter $\theta$,  and $\Theta$ has the inverse gamma distribution with parameters $\alpha$ and $\beta$.   Given sample data $x=(x_1, x_2, \ldots, x_n)$.  Compute the posterior distribution $s(\theta|t(x))$ up to constant factors. 

* [3 points] Identify the distribution of $s(\theta|t(x))$.

*  [4 points] Now *pretend* you could not identify it, and could not deduce exact constant factors.   For the simple case, where $\alpha =2$,  $\beta=3$, $n=3$, and  $x_1=2, x_2=4, x_3=6$, sample from $s(\theta|t(x))$ using the Metropolis algorithm. 

*  [3 points] Plot independent samples in a probability histogram and compare with the true result.





## 3.) [15 points] A Poisson process process on a perimeter of a semi-circle

Let $\Gamma$ be  a homogeneous Poisson point process of intensity $2$ on the upper half of the circle given by $x^2+y^2 =1$.  Here, $\Gamma$ is not the Gamma function.   Consider the point process $\Upsilon$ given by the  projection of $\Gamma$ onto the $x$-axis; that is, if $\Gamma$ had $n$ points and they are given by $(x_1, y_1), \ldots, (x_n, y_n)$, then the points of  $\Upsilon$ are just the $x$-coordinates  $x_1, \ldots, x_n$.   

*  [5 points] Write code to simulate $\Gamma$ and $\Upsilon$.    Graphically display a sample realization of these point processes.

*   [5 points] Demonstrate using simulations that $\Upsilon$ is *not* a homogeneous Poisson point process on $[-1,1]$.

*  [5 points]  Show analytically that $\Upsilon$ cannot be a homogeneous Poisson point process on $[-1,1]$.
```{r}
#3.1
#function to define point on the upper circle
point <- function(){
  z <- 2
    x <- 2*runif(1) -1
    y <- sqrt(1-x^2)
    z <- c(x,y)
  z
}
#replicate to create multiple points
re <- replicate(rpois(1,2*pi),point())

#plot upper half circle for ease of interpretation
curve(sqrt(1-x^2), from=-1, to=1 ,lty=2, xlab="x", ylab="y")

#plot points to get Gamma
points(re[1,], re[2,], xlim=c(-1.1,1.1), ylim=c(-0.1,1.1))

#plot zero line for ease of interpretation
abline(0,0,lty=2)

#plot to get projection onto x axis, this is Upsilon
points(re[1,],rep(0,length(re[1,])))

#3.2
#To show non-homogeneity, replicate this 1000 times, split into 2 intervals, take
#means, means should differ. See ICA 1.
```

## 4.)  [15 points] The transition rate matrix

You are given the the sample data from an irreducible  continuous-time Markov chain.  The sample data includes the jump times $(0,j_1, \ldots, j_n)$ and states $(s_0, s_1, \ldots, s_n)$; here at time $j_i$ the Markov chain jumps into state $s_i$ and stays there until the next jump which occurs at time $j_{i+1}$.

* [8 points]  When $n$ is large, give a method for estimating the transition rate matrix, also referred to as the $Q$ matrix.  Explain why your estimate is reasonable.

* [7 points] Import the data from the file [Q.txt](https://tsoo-math.github.io/ucl2/Q.txt) and use this data and your method above to estimate the $Q$ matrix.

```{r}
library(tidyverse)
data <- read.table(url("https://tsoo-math.github.io/ucl2/Q.txt"), header = TRUE, sep = ",") %>% 
  mutate(previous_state = lag(states)) %>% 
  mutate(jump_time = time - lag(time))
head(data)
# by running the below code we can see we have three states
state_list <- data %>%
  distinct(states) %>% 
  pull()
# what is the average time to go from state 1 to state 2?
for (i in state_list) {
  for (j in state_list){
    avg_time <- data %>% 
      filter(states == i & previous_state == j) %>% 
      group_by(states, previous_state) %>% 
      summarize(mean(jump_time), .groups = 'drop') %>% 
      ungroup() %>% 
      pull("mean(jump_time)")
    print(paste0("Average time spent jumping from state ", i, " to state ", j,
                 ": ", avg_time))
  }
}

```


##  5.) [10 points] Queues 

Suppose you  have Poisson arrivals, with intensity $6$.  You are given the following two options. Option 1: we treat it like a $M(6)/M(8)/1$ system- the items are served by exponentially at rate $8$.  Option 2:  each item is painted red or blue independently with probability $\tfrac{1}{2}$;  the coloured items report to different queues, with  the red items are served exponentially at rate $4$, and the blue items served exponentially at rate $4$.  

* [5 points] Run simulations to identify   the stationary distributions of the items in each of the two options.    Which option, on average, has more items in it?

*  [5 points] Which option is better, from the items/customers perspective?  Explain, analytically.

```{r}
opt <- function(t){
  inter = rexp(1,6)
  arr = inter
  
  while(t > arr[length(arr)]){
    inter <- c(inter, rexp(1,6))
    arr <- cumsum(inter)
  }
  
  L = length(inter)
  service = rexp(L,8)
  
  output <- arr[1] + service[1]
  for (i in 1:(L - 1)){ 
    if(arr[i + 1] < output[i]){output <- c(output, output[i] + service[i + 1])}
    if(arr[i + 1] > output[i]){output <- c(output, arr[i + 1] + service[i + 1])}
  }
  
  output <- output[-L]

  n = sum(output > t)
  
}

opt2 <- function(t){
  inter = rexp(1,6)
  arr = inter
  x <- sample(c(0,1), n, replace = TRUE, prob = c(0.5,0.5))
  if (x == 0){service1 = rexp(L, 4)}
  output1 <- arr[1] + service1[1]
  for (i in 1:(L - 1)){  
    if(arr[i + 1] < output1[i]){output1 <- c(output1, output1[i] + service1[i + 1])}
    if(arr[i + 1] > output1[i]){output1 <- c(output1, arr[i + 1] + service1[i + 1])}
  }
  if (x == 1){service2 = rexp(L, 4)}
  output2 <- arr[1] + service2[1]
  for (i in 1:(L - 1)){ 
    if(arr[i + 1] < output2[i]){output2 <- c(output2, output2[i] + service2[i + 1])}
    if(arr[i + 1] > output2[i]){output2 <- c(output2, arr[i + 1] + service2[i + 1])}
  }
  output1 <- output1[-L]
  output2 <- output2[-L]
  
  n1 = sum(output1 > t)
  n2 = sum(output2 > t)
  c(n1,n2)
}
```

# Endnotes

* Version: `r format(Sys.time(), '%d %B %Y')`
* [Rmd Source](https://tsoo-math.github.io/ucl2/2021-ica3-stat9-release.Rmd)


