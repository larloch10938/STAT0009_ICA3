---
output:
  html_document: default
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
```
## Student Numbers
SN: 18014015
SN: 21169367
SN: 19011782
SN: 20117921

# Questions

## 1.) [10 points] Apery's constant 

* [2 points] Prove (using calculus) that

$$\zeta(3) = \sum_{n=1} ^{\infty} \frac{1}{n^3}  < \infty.$$

* [6 points]  Consider the probability distribution $\mu$, where 
$$\mu(n) = \frac{1}{\zeta(3)n^3}.$$
Use the Metropolis algorithm to sample from $\mu$, so that you do not need to know the value of $\zeta(3)$.

* [2 points]  Now that you can sample from $\mu$, use simulations to estimate the value of $\zeta(3)$.  


## Answer:
## 1.1
* We can use a Standard Series convergence test called the Integral Test (learnt in MATH0004 Analysis 2 in 1st year at UCL), which looks at the series as an integral (an integral is the continuous version of a discrete sum) from 1 to $\infty$. If this definite integral converges then the series converges:
$$\int_{1}^{\infty} \frac{1}{x^{3}} \,dx \ = \lim_{t \to \infty}\int_{1}^{t} \frac{1}{x^{3}} \,dx = \lim_{t \to \infty}  \frac{1}{-2x^{2}}\Biggr|_{x=1}^{x=t} = \frac{1}{2} < \infty $$
The integral is less than $\infty$, thus $\zeta(3)$ is a convergent series as required.


## 1.2
* From Week 9 Exercises in class, we were introduced to the Metropolis-Hastings algorithm. Here we are sampling from $\mu(n) = \frac{1}{\zeta(3)n^3}$ and we do not know the value of the constant $\zeta(3)$. With the Metropolis-Hastings algorithm we select a proposal distribution $Q$, where $Q(j|i)=Q_{ij}$, a transition matrix. 


The Markov Chain we consider advance one step in the following way:
* If we are at a state $i$, such that $X_n=i$, then we generate a random variable $Y=j$ with distribution $Q(\cdot|i)$.

* Given that $Y=j$, we move to state $j$, so that $X_{n+1} = j$,  with probability
$$ \alpha(j|i) = \min\big( \tfrac{{\pi}(j)}{{\pi}(i)} \cdot \tfrac{Q(i|j)}{Q(j|i)},1\big);$$ Where $\pi(n) = \frac{1}{n^3}, our target distribution to a constant.

*  Otherwise, we stay in state $i$, so that $X_{n+1} = i$.

If we do move to state $j$ the proposal is sometimes refered to as being *accepted*; we can actually have some freedom to choose $\alpha$, and the choice above is sometimes referred to as the Metropolis choice. The probability of moving between state $i$ to state $j$ can be simulated by a Bernoulli trial with the probability of $\alpha(j|i)$.
These steps are repeated a large number of time to generate the Markov chain and obtain the sample distribution of $\mu(n)$, outlined in the code below.



```{r}
## Aperys constant and the plot of points for the actual distribution for verification later 
s = 0
points = c()
for (i in 1:1000){
  s <- s + 1/(i^3)
}
for (x in 1:7){
  points <- c(points, 1/(s*(x^3)))
}



#Construction of Metropolis algorithm based off week 9 exercises
function_pi <- function(n){ #pi function is the target distribution we have up to a constant
 z=0
  if(n >0 && n == floor(n)){z = 1/(n^3)} #target distribution, as it is a discrete distribution for positive integers, we ensure the input is this
 z
  }


Yq <- function(x){   #Yq function calculates the proposal distribution for each x
   y= 1 + rgeom(1,x)     #As we are dealing with a discrete distribution, a good idea would be to use a discrete distribution for our proposed distribution Q. Here, we shall                            use the geometric distribution as it only outputs positive integers and can help us explore higher states than our initial due to its unboundness.
 y
}


alpha <- function(j,i){ #alpha function works out the probability of moving from state i to state j
  prob_i = 1/(2*i)  #Here, i is the state of the current markov chain with the target distribution and j is state of the next potential step of the markov chain with the target   distribution.
  prob_j = 1/(2*j)    # The ratios are calculated and the minimum is taken with 1 to in find the probability #of moving from state i to state j
  m=min( c( function_pi(j)/function_pi(i)*dgeom(i-1, prob_j)/dgeom(j-1, prob_i), 1)) 
  m          

}

metro <- function(N){ #the Metropolis algorithm function combining everything together to find the samples of the mu distribution, simulates the Markov chain up to N
  x=1                 #During each iteration, j is calculated from the geometric distribution using the state i. The probability of acceptance is calculated by the alpha 
  for(k in 1:N){      #function and is then used in a Bernoulli trial (as indicated in the description of the algorithm) to represent the probability of that the next step of 
    i = x[length(x)]  #the Markov chain is j. This is then repeated for the each time step.
    prob_i = 1/(2*i)
    j = Yq(prob_i)
    p = alpha(j,i) 
    x = c(x,i)
      if ( rbinom(1,1,p)==1 ) {
      x[length(x)] <- j
            }
  }
  x
}


n = replicate(1000, metro(1000)) #run the algorithm 1000 times to converge to the target distribution

#Comparison to actual distribution, hence the use of the probability histogram

X= seq(1, 7, by =1)
hist(n, prob=TRUE, breaks=500, xlim=c(0,7), xlab = "n", ylab= expression(mu(n)),  main="Sample distribution of Mu") #plot of the distribution as probability histogram
lines(X, points, col="red",  lty= "longdash") #the actual distribution as verification with Apery's constant
legend("topright", legend=c("Sampling from Metropolis-Hastings","Apery's constant distribution"), col=c("gray", "red"), lty=c(1,1))

#The curve of the actual distribution and the histogram have roughly the same shape, verifying by simulations that they are the same 
```

##1.3
* To estimate Apery's constant with the sampling from the Metropolis-Hastings algorithm, we could simulate the distribution for all $n>0, n \in\mathbb{Z^{+}}$ and then find the sample mean of the distribution which would equate to (using the knowledge derived by Euler that $\sum_{n=1}^{\infty} \frac{1}{n^{2}}=\frac{\pi^{2}}{6}$. Upon, trying this we didn't get close to Apery's constant.
After, we thought of using the sampling distribution knowledge obtained in the previous part. We had the distribution approximated for all the positive integers, so we could take any of them, use the Law of Large numbers with the original distribution formula and rearrange to obtain an approximation of Apery's constant. To do this we can rearrange the formula of the mu distribution for Apery's constant and divide it by number of iterations (N) squared to get the approximation by Law of Large numbers. (The number of iterations is squared as it is N simulations each with a Markov chain with N time-steps)


For example, we could let $n=2$ and find out $\mu(2) from the simulations, N=1000 and rearrange the formula,
$$ \zeta(3) \approx \frac{1}{2^{3}(1000)^2\mu(2)}$$

We can use the same idea across the distribution. (e.g n=1, see below)

USE SAMPLE MEAN OR REGULAR MEAN FROM PART 2 OF QUESTION 1?

```{r}
## Process to get an approximation for Apery's constant with 1000 simulations from part Q1.2
d= replicate(1000, metro(1000)) #metropolis algorithm ran for 1000 iterations
t <-table(d) #table of the integer values and their corresponding occurrences
aperys_for_two = 1/(t[2]/1000^2* 2^3) #computation for n=2 
aperys_for_one = 1/(t[1]/1000^2* 1^3) #computation for n=1
aperys_for_two # output
aperys_for_one
```



## 2.) [15 points] Bayesian statistics and MCMC

We say that a positive continuous random variable $X$ has the **inverse gamma** distribution with parameters $\alpha >0$ and $\beta >0$ if it has pdf given by
	$$(y; \alpha, \beta) \mapsto \frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{-\alpha -1} e^{\tfrac{-\beta}{y}}  \mathbf{1}[y >0],$$ where $\Gamma$ is the usual Gamma function.  	  

We say that a positive continuous random variable $W$ has the **Scaled-Weibull distribution** with shape parameter $k$ and scale  parameter $\theta >0$ if it has pdf given by
$$(w_1; k,\theta) \mapsto \mathbf{1}[w_1 >0]\frac{k w_1^{k-1}}{\theta}   \exp[  - \tfrac{w_1^{k}}{\theta }  ] .$$ 	

* [2 points] Let ${W} = (W_1, \ldots, W_n)$ be a random sample from the  Scaled-Weibull distribution with known shape parameter $k$ and  unknown scale parameter $\theta >0$.  Show that $t({W}) := \sum_{i=1} ^n W_i ^k$ is a sufficient statistic for $\theta$.
	
* [3 points] Fix $k >0$.   Let ${X} = (X_1, \ldots, X_n)$ be a random sample where the conditional distribution of $X_1$ given $\Theta = \theta$  has the  Scaled-Weibull distribution with shape  parameter $k$ and scale parameter $\theta$,  and $\Theta$ has the inverse gamma distribution with parameters $\alpha$ and $\beta$.   Given sample data $x=(x_1, x_2, \ldots, x_n)$.  Compute the posterior distribution $s(\theta|t(x))$ up to constant factors. 

* [3 points] Identify the distribution of $s(\theta|t(x))$.

*  [4 points] Now *pretend* you could not identify it, and could not deduce exact constant factors.   For the simple case, where $\alpha =2$,  $\beta=3$, $n=3$, and  $x_1=2, x_2=4, x_3=6$, sample from $s(\theta|t(x))$ using the Metropolis algorithm. 

*  [3 points] Plot independent samples in a probability histogram and compare with the true result.

## Answer:

*  The likelihood function of $\theta$ is $$L(\theta,k,w_i) = \prod_{i=1}^{n}\mathbf{1}[w_i >0]\frac{k w_i^{k-1}}{\theta}\exp[- \tfrac{w_i^{k}}{\theta }] = \mathbf{1}[w_{(1)} >0]k^n (\prod_{i=1}^{n} w_i^{k-1})\theta^{-n}\exp[- \tfrac{\sum_{i=1}^{n}w_i^{k}}{\theta }].$$ Then, if we define $t(W)$ as $\sum_{i=1}^{n}w_i^{k}$, we can rewrite the likelihood function as $$L(\theta,k,w_i) = \mathbf{1}[w_{(1)} >0]k^n (\prod_{i=1}^{n} w_i^{k-1})\theta^{-n}\exp[- \tfrac{t(W)}{\theta }]=g(t;\theta)*h(\underline{w}),$$ where $$g(t;\theta)=\theta^{-n}exp[-\tfrac{t(W)}{\theta}]$$ and $$h(\underline{w})=\mathbf{1}[w_{(1)}>0]k^n\prod_{i=1}^{n}w_i^{k-1}.$$ Using the factorisation theorem, we can see that  $t(W)=\sum_{i=1}^{n}w_i^{k}$ is indeed a sufficient statistic of $\theta$.

*  It is known that the posterior distribution is proportional to the product of the prior distribution and the likelihood function. Therefore, $s(\theta|t(x))$ is proportional to $$\Pi(\Theta)\times L(\underline{x}|\Theta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{-\alpha -1} e^{\tfrac{-\beta}{\theta}}  \mathbf{1}[\theta >0]\times\mathbf{1}[x_{(1)} >0]k^n (\prod_{i=1}^{n} x_i^{k-1})\theta^{-n}\exp[- \tfrac{t(x)}{\theta }]$$. This can be further simplified so that $$s(\theta|t(x)) = C \times\frac{\beta^{\alpha}}{\Gamma(\alpha)}k^n (\prod_{i=1}^{n} x_i^{k-1})\theta^{-(\alpha+n)-1}\exp[-\tfrac{\beta+t(x)}{\theta }],$$ where $\theta >0$ and $x=(x_1, x_2, \ldots, x_n)>0$ and $C$ is a proportionality constant that would make the expression integrate to 1 (so that it is a proper probability distribution function).

*  $s(\theta|t(x))$ follows an Inverse Gamma distribution with parameters $\alpha +n$ and $\beta + t(x)$. Therefore, the proportionality constant, C, must be such that $$C *\frac{\beta^{\alpha}}{\Gamma(\alpha)}k^n (\prod_{i=1}^{n} x_i^{k-1})= \frac{[\beta + t(x)]^{\alpha+n}}{\Gamma(\alpha +n)}.$$

```{r}
f <- function(x){                  #This is the target distribution known up to
  z=0                              #a constant factor, when a=2, b=3, n=3, x1=2, 
  if(x >0) {z=(x^(-5)*exp(-15/x))} #x2=4, x3=6, and k=1.
  z
}

Yq <- function(x){                 #This is a random variable Y, with proposal 
  y= rnorm(1,x,1)                  #distribution Q ~ Normal (x,1). A Normal
  y                                #distribution was chosen due to its ease of
}                                  #use.

alpha <- function(j,i){            #Alpha is the probability of moving from
  m=min( c( f(j)/f(i), 1 ))        #state i to state j, as defined in our lecture.
  m
}

metro <- function(n){             #This is a function that implements the 
  x=10   #starting value          #Metropolis-Hastings algorithm and samples
  for(k in 1:n){                  #from the target distribution. It simulates a
    i = x[length(x)]              #Markov Chain of length n, where at each current
    j = Yq(i)                     #state a next step is proposed which is either
    p = alpha(j,i)                #accepted or rejected based on alpha. A Bernoulli
    x = c(x,i)                    #trial with probability p is used to decide 
    if ( rbinom(1,1,p)==1 ) {     #whether the chain moves to the next state or 
      x[length(x)] <- j           #stays in the current one. Here, p represents
    }                             #the probability that the next state of the MC
  }                               #is j, given that the current one is i.
  x
}

z = replicate(1000, metro(1000))  #Then, we create 1000 samples.

#Then we plot a histogram of independent samples from the target distribution 
#(using the MH-algotrithm).

hist(z, prob=TRUE, breaks=50, xlab = expression(theta),ylab="Probability", 
     main="Posterior distribution of theta", ylim=c(0,0.4), xlim=c(0,50))
x= seq(0, 1, by =0.1)

#We add the curve of the Inverse Gamma (5,15) distribution (this is the true distribution #based on section 2.3).

curve(15^5/24*x^(-6)*exp(-15/x), add=T, col="red")
legend("center",legend=c("Inverse Gamma (5,15)", "Samples using MH"), 
       col=c("gray","red"),lty=c(1,1))

#The curve and the histogram follows roughly the same shape indicating that 
#the Metropolis-Hastings algorithm was indeed successful and that our guess of
#the posterior distribution being an Inverse Gamma distribution was correct.
```

## 3.) [15 points] A Poisson process  process on a perimeter of a semi-circle

Let $\Gamma$ be  a homogeneous Poisson point process of intensity $2$ on the upper half of the circle given by $x^2+y^2 =1$.  Here, $\Gamma$ is not the Gamma function.   Consider the point process $\Upsilon$ given by the  projection of $\Gamma$ onto the $x$-axis; that is, if $\Gamma$ had $n$ points and they are given by $(x_1, y_1), \ldots, (x_n, y_n)$, then the points of  $\Upsilon$ are just the $x$-coordinates  $x_1, \ldots, x_n$.   

*  [5 points] Write code to simulate $\Gamma$ and $\Upsilon$.    Graphically display a sample realization of these point processes.

*   [5 points] Demonstrate using simulations that $\Upsilon$ is *not* a homogeneous Poisson point process on $[-1,1]$.

*  [5 points]  Show analytically that $\Upsilon$ cannot be a homogeneous Poisson point process on $[-1,1]$.

## Answer:

#### 3.1
```{r}
#3.1
#First we write a function to define a point on the upper circle.
point <- function(){
    x <- 2*runif(1) -1
    y <- sqrt(1-x^2)
    z <- c(x,y)
  z
}
#Then, we replicate this to create multiple points.
re <- replicate(rpois(1,2*pi),point())

#Then, we plot an upper half circle for ease of interpretation.
curve(sqrt(1-x^2), from=-1, to=1 ,lty=2, xlab="x", ylab="y")

#Next, we plot points to get Gamma.
points(re[1,], re[2,], xlim=c(-1.1,1.1), ylim=c(-0.1,1.1))

#We plot a line y=0 for ease of interpretation.
abline(0,0,lty=2)

#We plot the projection of Gamma onto x axis, this is Upsilon.
points(re[1,],rep(0,length(re[1,])))
```

#### 3.2
Remembering the last exercise (shop keeper) from ICA1, this problem can be set up similarly, using the law of large numbers. The law of large numbers tells us that $${\frac {1}{n} (X_1+...+X_n) \rightarrow E[X_1]= \lambda (s-r) \; \text{as} \; n \rightarrow \infty},$$ where (s-r) is the length of the interval which $\lambda$ is estimated for. Therefore, the estimator $\frac {1}{n(s-r)} (X_1+...+X_n)$ is consistent and unbiased for $\lambda$, the true rate of the Poisson process on [r,s]. With this estimator and the sample data, we give the corresponding estimate $\frac {1}{n(s-r)} (x_1+...+x_n)$ for $\lambda$.

Then, if we repeat the point generating-process from *3.1* n-times for $\Upsilon$, and we let $X_1,...,X_n$ be random variables representing the number of points generated for each of the $n$ replicates, and given that we can assume that these random variables are independent, we can say that the model gives that $X_i$ is a Poisson random variable with mean $\lambda_i$.  

Thus, if $[-1,1]$ interval is split up into smaller intervals, and the rate ($\lambda_i$) of the Poisson process $X_i$ is estimated in each interval, the result should be different in each interval, if $\Upsilon$ is indeed a non-homogeneous Poisson point process on $[-1,1]$.

We can estimate $\lambda_i$, the rate of "arrival" in the $i^th$ interval for all i, where in our case $i=1,2,3$. Then, if the estimates in each interval are approximately equal, $\Upsilon$ is homogeneous, and if the estimates are unequal, $\Upsilon$ is non-homogeneous. This is because as $n \rightarrow \infty$, we know that our estimator of $\lambda_i$ will be unbiased and consistent. 

The below code chunk will execute this in R.
```{r}
#3.2
#First, we generated points for Upsilon 100 times and created a vector, "reps" to
#store all the points.
set.seed(123)
counter<- 0
reps <- 0
n <- 100
while(counter < n){
  counter <- counter+1
  reps <- append(reps,replicate(rpois(1,2*pi),point())[1,])
}

#Then, the function "nonhom" was defined, that splits the [-1,1] into 3 intervals at
#x and y, and estimates lambda_i in each interval according to the method described 
#above, then outputs these estimates.

#As it is not specified whether x<y or y<x in the function, the three intervals are
#[-1,min(x,y)], [min(x,y),max(x,y)], and [max(x,y),1].

nonhom <- function(x,y){     
          int1 <- reps[reps<min(x,y)]            #define points in first interval  
          int2 <- reps[min(x,y)<reps && reps <max(x,y)] #define points in second interval
          int3 <- reps[reps>max(x,y)]                   #define points third interval
          if (length(int1) > 0){
          numpoint1 <- length(int1)   #If there are points in the interval, count
          } else {                    #the number of points in it. 
            numpoint1 <- 0            #If the interval is empty, the number of points
          }                           #is 0.
          if (length(int2) > 0){      #Repeat this for all three intervals.
          numpoint2 <- length(int2)
          } else {
            numpoint2 <- 0
          }
          if (length(int3) > 0){
          numpoint3 <- length(int3)
          } else {
            numpoint3 <- 0
          }                           #Estimate each lambda according to the
                                      #formula defined above and print results.
          
          print(c(numpoint1*1/(n*(min(x,y)+1)),
                  numpoint2*1/(n*(abs(x-y))), 
                  numpoint3*1/(n*(1-max(x,y)))))
}

#The values -0.66 and 0.66 were randomly selected. The function works with any 
#other input x or y values as well. 

nonhom(-0.66,0.66) #this is lambda1, lambda2, and lambda3

#From the results it is clear that three lambda values are different. Due to the
#large sample size and the law of large numbers, we can say that this difference 
#in the lambda values is evidence against the Upsilon being homogeneous. Therefore,
#we have proven that Upsilon is a non-homogeneous Poisson point process.
```
#### 3.3
Prove analytically

## 4.)  [15 points] The transition rate matrix

You are given the the sample data from an irreducible  continuous-time Markov chain.  The sample data includes the jump times $(0,j_1, \ldots, j_n)$ and states $(s_0, s_1, \ldots, s_n)$; here at time $j_i$ the Markov chain jumps into state $s_i$ and stays there until the next jump which occurs at time $j_{i+1}$.

* [8 points]  When $n$ is large, give a method for estimating the transition rate matrix, also referred to as the $Q$ matrix.  Explain why your estimate is reasonable.

* [7 points] Import the data from the file [Q.txt](https://tsoo-math.github.io/ucl2/Q.txt) and use this data and your method above to estimate the $Q$ matrix.

## Answer:

When $n$ is large we can use the generated data to compute the average amount of time spent in a specific state. This is $h$.

When $n$ is large we can also compute the average probability of a jump from one state to the other. By computing this we can find an empiric definition of $M$.

Using the formula defined in our lessons (https://tsoo-math.github.io/ucl/continuous-timeMC.html) we can derive $q_{i,i}$.

Combing $M$ and the derived $q_{i,i}$ will eventually result in $Q$.

```{r, message=FALSE}
library(tidyverse)
m <- matrix(data=0,nrow=3,ncol=3)
q <- matrix(data=0,nrow=3,ncol=3)
```

```{r, message=FALSE}
data <- read.table(url("https://tsoo-math.github.io/ucl2/Q.txt"), header = TRUE, sep = ",") %>% 
  mutate(previous_state = lag(states)) %>% 
  mutate(jump_time = time - lag(time))
# by running the below code we can see we have three states
state_list <- data %>%
  distinct(states) %>% 
  pull()
# by running the below code we get the amount of time spent on each state
h <- data %>% 
  group_by(previous_state) %>% 
  summarize(mean(jump_time)) %>% 
  drop_na() %>% 
  pull('mean(jump_time)')
print(h)
# compute q_ii from h
q_ii <- -1 / h
print(q_ii)
# by running the below code we get the amount of times we jumped from state i to state j
m_data <- data %>% 
  group_by(states, previous_state) %>% 
  summarize(n = n())
for (i in state_list) {
  for (j in state_list) {
    if (i != j) {
      m[i, j] <- m_data %>% 
      filter(states == j) %>% 
      filter(previous_state == i) %>% 
      pull(n)
    }
  }
}
m <- t(apply(m, 1, function(x) x/sum(x)))
print(m)
for (i in state_list) {
  q[i, ] <- -m[i, ] * q_ii[i]
  q[i, i] <- q_ii[i]
}
print(q)
```


##  5.) [10 points] Queues 

Suppose you  have Poisson arrivals, with intensity $6$.  You are given the following two options. Option 1: we treat it like a $M(6)/M(8)/1$ system- the items are served by exponentially at rate $8$.  Option 2:  each item is painted red or blue independently with probability $\tfrac{1}{2}$;  the coloured items report to different queues, with  the red items are served exponentially at rate $4$, and the blue items served exponentially at rate $4$.  

* [5 points] Run simulations to identify   the stationary distributions of the items in each of the two options.    Which option, on average, has more items in it?

*  [5 points] Which option is better, from the items/customers perspective?  Explain, analytically.

The two options are very similar: both receive the arrivals at the same rate but serves them in a different way (i.e. as a $M(8)$ versus 2 $M(4)$). We can clearly see the results will be different.

Option 1 is clearly superior to Option 2 since the amount of expected customers will be the same and they will all be served in the same queue; as soon as one customer is exiting the queue the next one will be served. That is not true for option 2, which is therefore prone to dead times and will be less efficient.

If we treat Option 2 as a combination of two $M(3)/M(4)/1$ system given the probability to get a red or blue ball is independent we can compute the expected time in the system using Little's Law as:

$$E(W) = \frac{1}{\lambda - \mu} = \frac{1}{4 - 3} = 1$$
so given we have two queues and equal probabilty of being in one or the other (i.e. 0.5):

$$ E(W_{tot}) = E(W) * 0.5 + E(W) * 0.5 = 0.5 + 0.5 = 1 $$

In option 1 we have:

$$E(W) = \frac{1}{\lambda - \mu} = \frac{1}{8 - 6} = \frac{1}{2} = 0.5$$

```{r}
normal_queue <- function(n) {
  M_1 <- 6
  M_2 <- 8
  inter <- rexp(n, M_1)
  arr <- cumsum(inter)
  service <- rexp(n, M_2)
  output <- arr[1] + service[1]
  for (i in 1:(n - 1)) {
      output <- c(output, max(output[i], arr[i + 1]) + service[i + 1])
  }
  return(mean(output - arr))
}

mean(replicate(100, normal_queue(10000)))
```
The result is very close to 0.5 confirming our analytical results.

```{r}
split_queue <- function(n) {
  M_1 <- 6
  M_2 <- 4
  inter <- rexp(n, M_1)
  arr <- cumsum(inter)
  prob <- rbinom(n, 1, 0.5)
  arr_blue <- arr[prob == 0]
  arr_red <- arr[prob == 1]
  service_blue <- rexp(n, M_2)
  service_red <- rexp(n, M_2)
  output_blue <- arr_blue[1] + service_blue[1]
  output_red <- arr_red[1] + service_red[1]
  for (i in 1:(length(arr_blue) - 1)) {
      output_blue <- c(output_blue, max(output_blue[i], arr_blue[i + 1]) + service_blue[i + 1])
  }
  for (i in 1:(length(arr_red) - 1)) {
      output_red <- c(output_red, max(output_red[i], arr_red[i + 1]) + service_red[i + 1])
  }
  
  return(0.5 * mean(output_blue - arr_blue) + 0.5 * mean(output_red - arr_red))
}

mean(replicate(100, split_queue(10000)))
```
The result is very close to 1 confirming our analytical results.

```{r}
 opt <- function(t){
  inter = rexp(1,6)
  arr = inter
  
  while(t > arr[length(arr)]){  ## x[length(x)] is to extract the last data in x
    inter <- c(inter, rexp(1,6))
    arr <- cumsum(inter)
  }
  
  L = length(inter)
  service = rexp(L,8)
  
  output <- arr[1] + service[1]
  for (i in 1:(L - 1)){  ## whether the server's service time less than the customer arrival time 
    if(arr[i + 1] < output[i]){output <- c(output, output[i] + service[i + 1])}
    if(arr[i + 1] > output[i]){output <- c(output, arr[i + 1] + service[i + 1])}
  }
  
  output <- output[-L] ## delete output[L]

  n = sum(output > t)
  
}
x= replicate(1000, opt(600)) ## simulation by large number
b = seq(-1,max(x)+1, by=1)
hist(x, prob=TRUE, breaks=b) ## draw a diagrom to assume the distribution

## find the stationary distribution of the first option
mean(x) ## the average items in the queue of first option
```

```{r}
opt2 <- function(t){
  inter = rexp(1,6)
  arr = inter
  
  while(t > arr[length(arr)]){
    inter <- c(inter, rexp(1,6))
    arr <- cumsum(inter)
  }
  L = length(inter)
  ## items painted red or blue with probability 0.5
  colitem = sample(c(0,1), length(arr), replace = TRUE, prob = c(0.5,0.5))
  
  arrred <- 0
  arrblue <- 0
  for (i in 1:L) { ## list the colored items with their arrival time
    
    if(colitem[i] == 0){arrred <- c(arrred, arr[i])}
    if(colitem[i] == 1){arrblue <- c(arrblue, arr[i])}
  }     
  arrred
  arrblue
  
  Lr = length(arrred)
  servicered = rexp(Lr,4)
  
  outputred <- arrred[1] + servicered[1]
  for (j in 1:(Lr - 1)){## whether the server's service time less than the customer arrival time  
    if(arrred[j + 1] < outputred[j]){outputred <- c(outputred, outputred[j] + servicered[j + 1])}
    if(arrred[j + 1] > outputred[j]){outputred <- c(outputred, arrred[j + 1] + servicered[j + 1])}
  }
  
  Lb = length(arrblue)
  serviceblue = rexp(Lb,4)  
  outputblue <- arrblue[1] + serviceblue[1]
  for (m in 1:(Lb - 1)){## whether the server's service time less than the customer arrival time 
    if(arrblue[m + 1] < outputblue[m]){outputblue <- c(outputblue, outputblue[m] + serviceblue[m + 1])}
    if(arrblue[m + 1] > outputblue[m]){outputblue <- c(outputblue, arrblue[m + 1] + serviceblue[m + 1])}
  }
  outputblue <- outputblue[-Lb] ## delete output[Lb]
  

  nr = sum(outputred > t)
  nb = sum(outputblue > t)

  n = (nr+nb)
}

y= replicate(1000, opt2(600)) ## simulation by large number

a = seq(-1,max(y)+1, by=1)

hist(y, prob=TRUE, breaks=a) ## negative binormial (sum of two geometric distribution)

mean(y)  ## average items in queue in the second option

```
* Compared with mean value of items in two options, the option1 have value around 3, and option2
have value around 6. These value stands for the average number of items in the corresponding queues. From the value we can see that option2 have doubled items in it on average.

* From items/customers perspective, the option1 definitely is better. 
Since the items/customers' arrival time is the same in two options, we need only to consider the 
time of waiting they spend on the queue. To compare the average items/customers in the queue,
we can see clearly the time items/customers spend on waiting.

*Although the second options split the queue into two, on average, there are more items/customers 
in the queue which means the effectiveness is not as good as the first option. Therefore, 
items/customers spend more time waiting in option2.
Therefore, the option with one queue is better for items/customers.


# Endnotes

* Version: `r format(Sys.time(), '%d %B %Y')`
* [Rmd Source](https://tsoo-math.github.io/ucl2/2021-ica3-stat9-release.Rmd)


